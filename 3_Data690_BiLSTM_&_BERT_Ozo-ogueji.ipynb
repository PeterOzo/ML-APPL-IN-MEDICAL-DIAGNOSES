{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPY48r7j1TT9"
      },
      "source": [
        "# Continuation of the project; I did the baseline TF-IDF and I will continue to\n",
        "# build BiLSTM and ClinicalModernBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A7l95Q0XKni"
      },
      "source": [
        "### 0 . Shared helpers, preprocessor, metrics, plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSflthOHYdmJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. HELPERS, PREPROCESSOR, METRICS, DIRS"
      ],
      "metadata": {
        "id": "N8OaLiosGCh2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0oLDShyYdpI",
        "outputId": "e3d5178b-619e-40ca-fc06-db08c623fe8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device in deep-learning chunks: cuda\n",
            "Plots dir: /content/drive/MyDrive/clinical_project_cpu/deep_models/plots\n",
            "Calibration dir: /content/drive/MyDrive/clinical_project_cpu/deep_models/calibration\n",
            "Embeddings dir: /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings\n",
            "Sample combined text:\n",
            " ['TRIAGE: abdominal pain, vomiting', 'TRIAGE: anorexia', 'TRIAGE: abdominal pain, abdominal distention, bowel obstruction [SEP] RADIOLOGY: examination: chest (posteroanterior and lateral) indication: with shortness of breath// eval pneumonia comparison: prior dated findings: posteroanterior and lateral views of the chest provided. lungs are clear. clips are noted near the ge junction. 2 discrete metallic stents partially visualized in the right upper quadrant. there is negation 0 focal consolidation, effusion, or pneumothorax. there are negation 0 signs of congestion or edema. the cardiomediastinal silhouette is normal. imaged osseous structures are intact. negation 0 free air below the right hemidiaphragm is seen. impression: negation 0 acute intrathoracic process. indication: history: with liver tx sent here for evaluation of obstruction.// obstruction technique: supine and upright abdominal radiographs were obtained. comparison: findings: there are few mildly dilated gas-filled loops of small bowel negation 6 air-fluid levels seen, nonspecific. there is relative paucity of colonic gas. however, air and stool is seen in the rectum. negation 0 evidence of free air is seen. re-demonstrated portal vein stent and a pack artery stent. impression: nonspecific bowel gas pattern with wanted to air-filled borderline to mildly dilated loops of small bowel. air and stool is seen in the rectum but a paucity of bowel gas throughout the remainder of the colon. if clinical concern for bowel obstruction, computed tomography would provide further assessment. negation 0 evidence of free air.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CHUNK 0 – HELPERS, PREPROCESSOR, METRICS, DIRS\n",
        "\n",
        "import os, gc, time, math, random, re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, f1_score, precision_score, recall_score,\n",
        "    accuracy_score, confusion_matrix\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ------------------------------\n",
        "# Output directories\n",
        "# ------------------------------\n",
        "BASE_OUT_DIR = Path(\"/content/drive/MyDrive/clinical_project_cpu/deep_models\")\n",
        "PLOTS_DIR    = BASE_OUT_DIR / \"plots\"\n",
        "CALIB_DIR    = BASE_OUT_DIR / \"calibration\"\n",
        "EMB_DIR      = BASE_OUT_DIR / \"embeddings\"\n",
        "\n",
        "for d in [BASE_OUT_DIR, PLOTS_DIR, CALIB_DIR, EMB_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Device in deep-learning chunks:\", device)\n",
        "print(\"Plots dir:\", PLOTS_DIR)\n",
        "print(\"Calibration dir:\", CALIB_DIR)\n",
        "print(\"Embeddings dir:\", EMB_DIR)\n",
        "\n",
        "# ------------------------------\n",
        "# GPU memory helper\n",
        "# ------------------------------\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# ------------------------------\n",
        "# Clinical text preprocessor\n",
        "# ------------------------------\n",
        "class ClinicalTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Simple, domain-aware cleaner that:\n",
        "    - lowercases\n",
        "    - keeps alphanumerics and basic punctuation\n",
        "    - joins TRIAGE + RADIOLOGY with explicit markers\n",
        "    \"\"\"\n",
        "    def __init__(self, lowercase=True, strip_special=True, normalize_ws=True):\n",
        "        self.lowercase = lowercase\n",
        "        self.strip_special = strip_special\n",
        "        self.normalize_ws = normalize_ws\n",
        "\n",
        "    def _clean(self, text: str) -> str:\n",
        "        if pd.isna(text):\n",
        "            text = \"\"\n",
        "        text = str(text)\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "        if self.strip_special:\n",
        "            text = re.sub(r\"[^a-z0-9\\.\\,\\-\\?\\!\\:\\;\\(\\)\\/\\s]+\", \" \", text)\n",
        "        if self.normalize_ws:\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "\n",
        "    def join_triage_radiology(self, triage_text, radiology_text) -> str:\n",
        "        t = self._clean(triage_text)\n",
        "        r = self._clean(radiology_text)\n",
        "        parts = []\n",
        "        if t:\n",
        "            parts.append(f\"TRIAGE: {t}\")\n",
        "        if r:\n",
        "            parts.append(f\"RADIOLOGY: {r}\")\n",
        "        return \" [SEP] \".join(parts) if parts else \"\"\n",
        "\n",
        "    def __call__(self, row):\n",
        "        return self.join_triage_radiology(row.get(\"triage_text\", \"\"),\n",
        "                                          row.get(\"radiology_text\", \"\"))\n",
        "\n",
        "preprocessor = ClinicalTextPreprocessor()\n",
        "\n",
        "# add combined_text once here\n",
        "for df in [X_train, X_val, X_test]:\n",
        "    df[\"combined_text\"] = df.apply(preprocessor, axis=1)\n",
        "\n",
        "print(\"Sample combined text:\\n\", X_train[\"combined_text\"].head(3).tolist())\n",
        "\n",
        "# ------------------------------\n",
        "# Vocab builder + tokenizer for BiLSTM\n",
        "# ------------------------------\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(texts, vocab_size=10000, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for txt in texts:\n",
        "        for tok in txt.split():\n",
        "            counter[tok] += 1\n",
        "    # reserve 0: PAD, 1: UNK\n",
        "    most_common = [w for w, c in counter.items() if c >= min_freq]\n",
        "    most_common = sorted(most_common, key=lambda w: counter[w],\n",
        "                         reverse=True)[:vocab_size-2]\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for i, w in enumerate(most_common, start=2):\n",
        "        vocab[w] = i\n",
        "    return vocab\n",
        "\n",
        "def text_to_ids(text, vocab, max_len):\n",
        "    tokens = text.split()\n",
        "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    pad_len = max_len - len(ids)\n",
        "    ids = ids + [vocab[\"<PAD>\"]] * pad_len\n",
        "    attn_mask = [1]*min(len(tokens), max_len) + [0]*pad_len\n",
        "    return np.array(ids, dtype=np.int64), np.array(attn_mask, dtype=np.int64)\n",
        "\n",
        "# ------------------------------\n",
        "# Generic binary metrics helper\n",
        "# ------------------------------\n",
        "def binary_metrics(y_true, y_prob, threshold=0.5):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1  = f1_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec  = recall_score(y_true, y_pred)\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    spec = tn / (tn + fp + 1e-8)\n",
        "\n",
        "    return {\n",
        "        \"auc\": auc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"accuracy\": acc,\n",
        "        \"specificity\": spec,\n",
        "        \"confusion\": (tn, fp, fn, tp),\n",
        "    }\n",
        "\n",
        "# ------------------------------\n",
        "# Plot helpers\n",
        "# ------------------------------\n",
        "def plot_training_curves(history, model_name, out_dir=PLOTS_DIR):\n",
        "    \"\"\"\n",
        "    history: dict with keys \"train_loss\", \"val_loss\", \"val_auc\"\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"])+1)\n",
        "\n",
        "    # Loss\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"{model_name} – Loss\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    path_loss = out_dir / f\"{model_name}_loss.png\"\n",
        "    plt.savefig(path_loss, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # AUC\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(epochs, history[\"val_auc\"], marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Validation AUC\")\n",
        "    plt.title(f\"{model_name} – Validation AUC\")\n",
        "    plt.tight_layout()\n",
        "    path_auc = out_dir / f\"{model_name}_val_auc.png\"\n",
        "    plt.savefig(path_auc, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"[{model_name}] Saved loss + AUC curves to {out_dir}\")\n",
        "    return str(path_loss), str(path_auc)\n",
        "\n",
        "def plot_confusion_matrix(cm, model_name, out_dir=PLOTS_DIR):\n",
        "    tn, fp, fn, tp = cm\n",
        "    mat = np.array([[tn, fp],\n",
        "                    [fn, tp]])\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.heatmap(mat, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
        "                yticklabels=[\"True 0\", \"True 1\"])\n",
        "    plt.title(f\"{model_name} – Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    path_cm = out_dir / f\"{model_name}_confusion_matrix.png\"\n",
        "    plt.savefig(path_cm, dpi=150)\n",
        "    plt.close()\n",
        "    print(f\"[{model_name}] Saved confusion matrix to {path_cm}\")\n",
        "    return str(path_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MT-tkjcXF6k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. BiLSTM COMPACT ENCODER"
      ],
      "metadata": {
        "id": "zaoGlut2F697"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWnphZ4XYlss",
        "outputId": "78b901d1-cdfa-4974-91d8-ffee64464aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline TF-IDF AUC to beat: 0.818\n",
            "\n",
            "[BiLSTM] Building vocabulary...\n",
            "Vocab size (including PAD/UNK): 10000\n",
            "[BiLSTM] Epoch 1/6 Train loss=0.4793 | Val loss=0.4633 | Val AUC=0.8087\n",
            "[BiLSTM] Epoch 2/6 Train loss=0.4501 | Val loss=0.4538 | Val AUC=0.8196\n",
            "[BiLSTM] Epoch 3/6 Train loss=0.4381 | Val loss=0.4501 | Val AUC=0.8245\n",
            "[BiLSTM] Epoch 4/6 Train loss=0.4292 | Val loss=0.4483 | Val AUC=0.8252\n",
            "[BiLSTM] Epoch 5/6 Train loss=0.4191 | Val loss=0.4543 | Val AUC=0.8232\n",
            "[BiLSTM] Epoch 6/6 Train loss=0.4084 | Val loss=0.4586 | Val AUC=0.8205\n",
            "[BiLSTM] Training finished in 8.7 minutes. Best Val AUC=0.8252\n",
            "[BiLSTM] Saved loss + AUC curves to /content/drive/MyDrive/clinical_project_cpu/deep_models/plots\n",
            "\n",
            "[BiLSTM] TEST METRICS\n",
            "          auc: 0.8144\n",
            "           f1: 0.4889\n",
            "    precision: 0.7028\n",
            "       recall: 0.3748\n",
            "  specificity: 0.9391\n",
            "     accuracy: 0.7824\n",
            "  Confusion: TN=20296, FP=1317, FN=5195, TP=3115\n",
            "[BiLSTM] Saved confusion matrix to /content/drive/MyDrive/clinical_project_cpu/deep_models/plots/BiLSTM_confusion_matrix.png\n",
            "\n",
            "[BiLSTM] Improvement over TF-IDF baseline: -0.0036 AUC\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1 BiLSTM COMPACT ENCODER\n",
        "\n",
        "MAX_LEN_LSTM = 512\n",
        "VOCAB_SIZE   = 10000\n",
        "\n",
        "print(f\"Baseline TF-IDF AUC to beat: {baseline_auc:.3f}\")\n",
        "\n",
        "# 1) Build vocabulary on training combined text\n",
        "print(\"\\n[BiLSTM] Building vocabulary...\")\n",
        "vocab = build_vocab(X_train[\"combined_text\"].tolist(),\n",
        "                    vocab_size=VOCAB_SIZE, min_freq=2)\n",
        "print(f\"Vocab size (including PAD/UNK): {len(vocab)}\")\n",
        "\n",
        "# 2) Dataset for BiLSTM  (INCLUDES stay_id)\n",
        "class ClinicalLSTMDataset(Dataset):\n",
        "    def __init__(self, df, labels, vocab, max_len=512):\n",
        "        self.texts   = df[\"combined_text\"].tolist()\n",
        "        self.labels  = labels.values.astype(np.int64)\n",
        "        # prefer explicit stay_id column if present, else use index\n",
        "        if \"stay_id\" in df.columns:\n",
        "            self.stay_ids = df[\"stay_id\"].values.astype(np.int64)\n",
        "        else:\n",
        "            self.stay_ids = df.index.values.astype(np.int64)\n",
        "        self.vocab   = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        ids, attn = text_to_ids(text, self.vocab, self.max_len)\n",
        "        return {\n",
        "            \"input_ids\":      torch.tensor(ids,  dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attn, dtype=torch.float32),\n",
        "            \"label\":          torch.tensor(self.labels[idx],   dtype=torch.long),\n",
        "            \"stay_id\":        torch.tensor(self.stay_ids[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_ds_lstm = ClinicalLSTMDataset(X_train, y_train, vocab, max_len=MAX_LEN_LSTM)\n",
        "val_ds_lstm   = ClinicalLSTMDataset(X_val,   y_val,   vocab, max_len=MAX_LEN_LSTM)\n",
        "test_ds_lstm  = ClinicalLSTMDataset(X_test,  y_test,  vocab, max_len=MAX_LEN_LSTM)\n",
        "\n",
        "train_loader_lstm = DataLoader(train_ds_lstm, batch_size=32, shuffle=True)\n",
        "val_loader_lstm   = DataLoader(val_ds_lstm,   batch_size=64, shuffle=False)\n",
        "test_loader_lstm  = DataLoader(test_ds_lstm,  batch_size=64, shuffle=False)\n",
        "\n",
        "# 3) BiLSTM with attention + encode_text\n",
        "class BiLSTMEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Compact domain-aware encoder:\n",
        "    - Embedding + BiLSTM\n",
        "    - Attention pooling to get fixed-length embedding\n",
        "    - encode_text() returns [batch, hidden_dim*2] embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128,\n",
        "                 num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_dim*2, 1)   # binary\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        emb = self.embedding(input_ids)                      # [B, T, D]\n",
        "        lstm_out, _ = self.lstm(emb)                         # [B, T, 2H]\n",
        "\n",
        "        scores = self.attn(lstm_out).squeeze(-1)             # [B, T]\n",
        "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "        weights = torch.softmax(scores, dim=-1)              # [B, T]\n",
        "        ctx = torch.bmm(weights.unsqueeze(1), lstm_out)      # [B, 1, 2H]\n",
        "        ctx = ctx.squeeze(1)                                 # [B, 2H]\n",
        "        return ctx\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        ctx = self.encode_text(input_ids, attention_mask)\n",
        "        logits = self.classifier(self.dropout(ctx)).squeeze(-1)   # [B]\n",
        "        return logits\n",
        "\n",
        "# 4) Training loop (logs loss + AUC)\n",
        "def train_lstm_model(model, train_loader, val_loader,\n",
        "                     n_epochs=6, lr=3e-4, weight_decay=0.02):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                  weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n",
        "    best_auc = 0.0\n",
        "    best_state = None\n",
        "    patience = 3\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            ids   = batch[\"input_ids\"].to(device)\n",
        "            mask  = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(ids, mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * ids.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # ----- validation -----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_probs, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                ids   = batch[\"input_ids\"].to(device)\n",
        "                mask  = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "                logits = model(ids, mask)\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item() * ids.size(0)\n",
        "\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "                all_labels.append(batch[\"label\"].numpy())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        all_probs = np.concatenate(all_probs)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        metrics = binary_metrics(all_labels, all_probs)\n",
        "        val_auc = metrics[\"auc\"]\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_auc\"].append(val_auc)\n",
        "\n",
        "        print(f\"[BiLSTM] Epoch {epoch}/{n_epochs} \"\n",
        "              f\"Train loss={train_loss:.4f} | Val loss={val_loss:.4f} \"\n",
        "              f\"| Val AUC={val_auc:.4f}\")\n",
        "\n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            best_state = model.state_dict()\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"[BiLSTM] Early stopping – no improvement.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, history, best_auc\n",
        "\n",
        "# 5) Train BiLSTM\n",
        "clear_gpu_memory()\n",
        "bilstm_model = BiLSTMEncoder(vocab_size=len(vocab), embed_dim=128,\n",
        "                             hidden_dim=128, num_layers=2, dropout=0.2)\n",
        "\n",
        "t0 = time.time()\n",
        "bilstm_model, history_lstm, best_val_auc_lstm = train_lstm_model(\n",
        "    bilstm_model,\n",
        "    train_loader_lstm,\n",
        "    val_loader_lstm,\n",
        "    n_epochs=6,\n",
        "    lr=3e-4,\n",
        "    weight_decay=0.02,\n",
        ")\n",
        "elapsed = (time.time() - t0) / 60.0\n",
        "print(f\"[BiLSTM] Training finished in {elapsed:.1f} minutes. \"\n",
        "      f\"Best Val AUC={best_val_auc_lstm:.4f}\")\n",
        "\n",
        "plot_training_curves(history_lstm, model_name=\"BiLSTM\")\n",
        "\n",
        "# 6) Evaluate on TEST and plot confusion matrix + full metrics\n",
        "def evaluate_lstm_on_test(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids   = batch[\"input_ids\"].to(device)\n",
        "            mask  = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].numpy()\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_probs  = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    metrics = binary_metrics(all_labels, all_probs, threshold=0.5)\n",
        "\n",
        "    print(\"\\n[BiLSTM] TEST METRICS\")\n",
        "    for k in [\"auc\", \"f1\", \"precision\", \"recall\",\n",
        "              \"specificity\", \"accuracy\"]:\n",
        "        print(f\"  {k:>11}: {metrics[k]:.4f}\")\n",
        "    tn, fp, fn, tp = metrics[\"confusion\"]\n",
        "    print(f\"  Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    plot_confusion_matrix(metrics[\"confusion\"], model_name=\"BiLSTM\")\n",
        "    return all_labels, all_probs, metrics\n",
        "\n",
        "y_test_lstm, y_prob_lstm, metrics_lstm = evaluate_lstm_on_test(\n",
        "    bilstm_model, test_loader_lstm\n",
        ")\n",
        "\n",
        "print(f\"\\n[BiLSTM] Improvement over TF-IDF baseline: \"\n",
        "      f\"{metrics_lstm['auc'] - baseline_auc:+.4f} AUC\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpLyfLPRY0YJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ClinicalModernBERT ENCODER"
      ],
      "metadata": {
        "id": "FGRar_jDFmSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXbDrj64Y0uT",
        "outputId": "e772743b-3673-4040-9790-1f8e1a72ee28"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Clinical BERT] Loading tokenizer & base model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at simonlee711/clinical_modernbert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Clinical BERT] Epoch 1/5 Train loss=0.4651 | Val loss=0.4530 | Val AUC=0.8200\n",
            "[Clinical BERT] Epoch 2/5 Train loss=0.4302 | Val loss=0.4446 | Val AUC=0.8267\n",
            "[Clinical BERT] Epoch 3/5 Train loss=0.3825 | Val loss=0.4636 | Val AUC=0.8180\n",
            "[Clinical BERT] Epoch 4/5 Train loss=0.2895 | Val loss=0.5454 | Val AUC=0.8000\n",
            "[Clinical BERT] Epoch 5/5 Train loss=0.2173 | Val loss=0.7089 | Val AUC=0.7747\n",
            "[Clinical BERT] Early stopping – no improvement.\n",
            "[Clinical BERT] Training finished in 163.1 minutes. Best Val AUC=0.8267\n",
            "[ClinicalBERT] Saved loss + AUC curves to /content/drive/MyDrive/clinical_project_cpu/deep_models/plots\n",
            "\n",
            "[Clinical BERT] TEST METRICS\n",
            "          auc: 0.7722\n",
            "           f1: 0.5374\n",
            "    precision: 0.6020\n",
            "       recall: 0.4853\n",
            "  specificity: 0.8766\n",
            "     accuracy: 0.7680\n",
            "  Confusion: TN=18947, FP=2666, FN=4277, TP=4033\n",
            "[ClinicalBERT] Saved confusion matrix to /content/drive/MyDrive/clinical_project_cpu/deep_models/plots/ClinicalBERT_confusion_matrix.png\n",
            "\n",
            "[Clinical BERT] Improvement over TF-IDF baseline: -0.0458 AUC\n",
            "\n",
            "[Task 6] Best deep model for Task 7: BiLSTM (AUC=0.8144)\n"
          ]
        }
      ],
      "source": [
        "# CHUNK 2 – TASK 6: ClinicalModernBERT ENCODER\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "BERT_MODEL_NAME = \"simonlee711/clinical_modernbert\"  # same as before\n",
        "MAX_LEN_BERT    = 256\n",
        "BATCH_SIZE_BERT = 16\n",
        "\n",
        "print(\"\\n[Clinical BERT] Loading tokenizer & base model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "bert_base = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "class ClinicalBertEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Compact encoder on top of ModernBERT:\n",
        "    - mean pooling over last hidden states (with mask)\n",
        "    - encode_text() returns [B, hidden_dim]\n",
        "    \"\"\"\n",
        "    def __init__(self, bert_model, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        last_hidden = outputs.last_hidden_state   # [B, T, H]\n",
        "        mask = attention_mask.unsqueeze(-1)       # [B, T, 1]\n",
        "        summed  = torch.sum(last_hidden * mask, dim=1)   # [B, H]\n",
        "        counts  = torch.clamp(mask.sum(dim=1), min=1e-9) # [B, 1]\n",
        "        mean_pooled = summed / counts\n",
        "        return mean_pooled\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        emb = self.encode_text(input_ids, attention_mask)\n",
        "        logits = self.classifier(self.dropout(emb)).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "# Dataset includes stay_id\n",
        "class ClinicalBertDataset(Dataset):\n",
        "    def __init__(self, df, labels, tokenizer, max_len=256):\n",
        "        self.texts    = df[\"combined_text\"].tolist()\n",
        "        self.labels   = labels.values.astype(np.int64)\n",
        "        if \"stay_id\" in df.columns:\n",
        "            self.stay_ids = df[\"stay_id\"].values.astype(np.int64)\n",
        "        else:\n",
        "            self.stay_ids = df.index.values.astype(np.int64)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len   = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"label\":          torch.tensor(self.labels[idx],   dtype=torch.long),\n",
        "            \"stay_id\":        torch.tensor(self.stay_ids[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_ds_bert = ClinicalBertDataset(X_train, y_train, tokenizer,\n",
        "                                    max_len=MAX_LEN_BERT)\n",
        "val_ds_bert   = ClinicalBertDataset(X_val,   y_val,   tokenizer,\n",
        "                                    max_len=MAX_LEN_BERT)\n",
        "test_ds_bert  = ClinicalBertDataset(X_test,  y_test,  tokenizer,\n",
        "                                    max_len=MAX_LEN_BERT)\n",
        "\n",
        "train_loader_bert = DataLoader(train_ds_bert, batch_size=BATCH_SIZE_BERT,\n",
        "                               shuffle=True)\n",
        "val_loader_bert   = DataLoader(val_ds_bert,   batch_size=BATCH_SIZE_BERT*2,\n",
        "                               shuffle=False)\n",
        "test_loader_bert  = DataLoader(test_ds_bert,  batch_size=BATCH_SIZE_BERT*2,\n",
        "                               shuffle=False)\n",
        "\n",
        "def train_bert_model(model, train_loader, val_loader,\n",
        "                     n_epochs=5, lr=2e-5, weight_decay=0.01):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
        "                                  weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n",
        "    best_auc = 0.0\n",
        "    best_state = None\n",
        "    patience = 3\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            ids   = batch[\"input_ids\"].to(device)\n",
        "            mask  = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(ids, mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * ids.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # ----- validation -----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_probs, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                ids   = batch[\"input_ids\"].to(device)\n",
        "                mask  = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "                logits = model(ids, mask)\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item() * ids.size(0)\n",
        "\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "                all_labels.append(batch[\"label\"].numpy())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        all_probs  = np.concatenate(all_probs)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        metrics = binary_metrics(all_labels, all_probs)\n",
        "        val_auc = metrics[\"auc\"]\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_auc\"].append(val_auc)\n",
        "\n",
        "        print(f\"[Clinical BERT] Epoch {epoch}/{n_epochs} \"\n",
        "              f\"Train loss={train_loss:.4f} | Val loss={val_loss:.4f} \"\n",
        "              f\"| Val AUC={val_auc:.4f}\")\n",
        "\n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            best_state = model.state_dict()\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"[Clinical BERT] Early stopping – no improvement.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history, best_auc\n",
        "\n",
        "clear_gpu_memory()\n",
        "bert_model = ClinicalBertEncoder(bert_base, dropout=0.3)\n",
        "\n",
        "t0 = time.time()\n",
        "bert_model, history_bert, best_val_auc_bert = train_bert_model(\n",
        "    bert_model,\n",
        "    train_loader_bert,\n",
        "    val_loader_bert,\n",
        "    n_epochs=5,\n",
        "    lr=2e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "elapsed = (time.time() - t0) / 60.0\n",
        "print(f\"[Clinical BERT] Training finished in {elapsed:.1f} minutes. \"\n",
        "      f\"Best Val AUC={best_val_auc_bert:.4f}\")\n",
        "\n",
        "plot_training_curves(history_bert, model_name=\"ClinicalBERT\")\n",
        "\n",
        "def evaluate_bert_on_test(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids   = batch[\"input_ids\"].to(device)\n",
        "            mask  = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(batch[\"label\"].numpy())\n",
        "\n",
        "    all_probs  = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    metrics = binary_metrics(all_labels, all_probs, threshold=0.5)\n",
        "\n",
        "    print(\"\\n[Clinical BERT] TEST METRICS\")\n",
        "    for k in [\"auc\", \"f1\", \"precision\", \"recall\",\n",
        "              \"specificity\", \"accuracy\"]:\n",
        "        print(f\"  {k:>11}: {metrics[k]:.4f}\")\n",
        "    tn, fp, fn, tp = metrics[\"confusion\"]\n",
        "    print(f\"  Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    plot_confusion_matrix(metrics[\"confusion\"], model_name=\"ClinicalBERT\")\n",
        "    return all_labels, all_probs, metrics\n",
        "\n",
        "y_test_bert, y_prob_bert, metrics_bert = evaluate_bert_on_test(\n",
        "    bert_model, test_loader_bert\n",
        ")\n",
        "print(f\"\\n[Clinical BERT] Improvement over TF-IDF baseline: \"\n",
        "      f\"{metrics_bert['auc'] - baseline_auc:+.4f} AUC\")\n",
        "\n",
        "# Decide best deep model for Task 7\n",
        "if metrics_bert[\"auc\"] >= metrics_lstm[\"auc\"]:\n",
        "    best_model_name = \"ClinicalBERT\"\n",
        "    best_model = bert_model\n",
        "    best_y_true = y_test_bert\n",
        "    best_y_prob = y_prob_bert\n",
        "else:\n",
        "    best_model_name = \"BiLSTM\"\n",
        "    best_model = bilstm_model\n",
        "    best_y_true = y_test_lstm\n",
        "    best_y_prob = y_prob_lstm\n",
        "\n",
        "print(f\"\\n[Task 6] Best deep model for Task 7: {best_model_name} \"\n",
        "      f\"(AUC={max(metrics_bert['auc'], metrics_lstm['auc']):.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-nJZ1gbb0Af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. CALIBRATION & RELIABILITY"
      ],
      "metadata": {
        "id": "kWFiolokCv88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "rQJLV4JQYlwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4874e788-3d83-4017-85b5-59c4ddac1679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Calibration] Optimal temperature: 1.081\n",
            "\n",
            "[Calibration] TEST comparison (AUC & F1):\n",
            "  Uncalibrated AUC: 0.8144, F1: 0.4889\n",
            "  Temp-scaled  AUC: 0.8144, F1: 0.4889\n",
            "[Calibration] Saved reliability plot to /content/drive/MyDrive/clinical_project_cpu/deep_models/calibration/BiLSTM_reliability_uncal.png\n",
            "[Calibration] Saved reliability plot to /content/drive/MyDrive/clinical_project_cpu/deep_models/calibration/BiLSTM_reliability_temp_scaled.png\n"
          ]
        }
      ],
      "source": [
        "# CHUNK 3 – TASK 7: CALIBRATION & RELIABILITY\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    \"\"\"Temperature scaling on logits.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.log_T = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, logits):\n",
        "        T = torch.exp(self.log_T)\n",
        "        return logits / T\n",
        "\n",
        "def fit_temperature(model, val_loader):\n",
        "    model.eval()\n",
        "    logits_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            ids   = batch[\"input_ids\"].to(device)\n",
        "            mask  = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "            logits = model(ids, mask)\n",
        "            logits_list.append(logits)\n",
        "            labels_list.append(labels)\n",
        "    logits = torch.cat(logits_list)\n",
        "    labels = torch.cat(labels_list)\n",
        "\n",
        "    scaler = TemperatureScaler().to(device)\n",
        "    optimizer = torch.optim.LBFGS([scaler.log_T], lr=0.01, max_iter=50)\n",
        "    nll_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _closure():\n",
        "        optimizer.zero_grad()\n",
        "        scaled_logits = scaler(logits)\n",
        "        loss = nll_criterion(scaled_logits, labels)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(_closure)\n",
        "    with torch.no_grad():\n",
        "        T = torch.exp(scaler.log_T).item()\n",
        "    print(f\"[Calibration] Optimal temperature: {T:.3f}\")\n",
        "    return scaler\n",
        "\n",
        "def reliability_plot(y_true, y_prob, title, out_path):\n",
        "    prob_true, prob_pred = calibration_curve(\n",
        "        y_true, y_prob, n_bins=10, strategy=\"quantile\"\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot([0,1], [0,1], \"k--\", label=\"Perfectly calibrated\")\n",
        "    plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
        "    plt.xlabel(\"Predicted probability\")\n",
        "    plt.ylabel(\"Observed frequency\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "    print(f\"[Calibration] Saved reliability plot to {out_path}\")\n",
        "\n",
        "# choose loaders for best model\n",
        "if best_model_name == \"ClinicalBERT\":\n",
        "    val_loader_best  = val_loader_bert\n",
        "    test_loader_best = test_loader_bert\n",
        "else:\n",
        "    val_loader_best  = val_loader_lstm\n",
        "    test_loader_best = test_loader_lstm\n",
        "\n",
        "clear_gpu_memory()\n",
        "best_model = best_model.to(device)\n",
        "temp_scaler = fit_temperature(best_model, val_loader_best)\n",
        "\n",
        "# apply calibration to TEST logits\n",
        "best_model.eval()\n",
        "logits_uncal, logits_cal, labels_test = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_best:\n",
        "        ids   = batch[\"input_ids\"].to(device)\n",
        "        mask  = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].numpy()\n",
        "\n",
        "        logits = best_model(ids, mask)\n",
        "        logits_uncal.append(logits.cpu().numpy())\n",
        "        logits_cal.append(temp_scaler(logits).cpu().numpy())\n",
        "        labels_test.append(labels)\n",
        "\n",
        "logits_uncal = np.concatenate(logits_uncal).squeeze()\n",
        "logits_cal   = np.concatenate(logits_cal).squeeze()\n",
        "labels_test  = np.concatenate(labels_test)\n",
        "\n",
        "probs_uncal = 1 / (1 + np.exp(-logits_uncal))\n",
        "probs_cal   = 1 / (1 + np.exp(-logits_cal))\n",
        "\n",
        "metrics_uncal = binary_metrics(labels_test, probs_uncal)\n",
        "metrics_cal   = binary_metrics(labels_test, probs_cal)\n",
        "\n",
        "print(\"\\n[Calibration] TEST comparison (AUC & F1):\")\n",
        "print(f\"  Uncalibrated AUC: {metrics_uncal['auc']:.4f}, \"\n",
        "      f\"F1: {metrics_uncal['f1']:.4f}\")\n",
        "print(f\"  Temp-scaled  AUC: {metrics_cal['auc']:.4f}, \"\n",
        "      f\"F1: {metrics_cal['f1']:.4f}\")\n",
        "\n",
        "# Reliability plots\n",
        "reliability_plot(\n",
        "    labels_test,\n",
        "    probs_uncal,\n",
        "    title=f\"{best_model_name} – Reliability (Uncalibrated)\",\n",
        "    out_path=CALIB_DIR / f\"{best_model_name}_reliability_uncal.png\",\n",
        ")\n",
        "reliability_plot(\n",
        "    labels_test,\n",
        "    probs_cal,\n",
        "    title=f\"{best_model_name} – Reliability (Temp-scaled)\",\n",
        "    out_path=CALIB_DIR / f\"{best_model_name}_reliability_temp_scaled.png\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2R-mcgGCrOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ROBUSTNESS & SIMPLE RATIONALES"
      ],
      "metadata": {
        "id": "JJ9m7RCiDQjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBUSTNESS & SIMPLE RATIONALES\n",
        "\n",
        "import copy\n",
        "\n",
        "# ---------- simple text augmentations ----------\n",
        "def simple_paraphrase(text: str) -> str:\n",
        "    repl = {\n",
        "        \"shortness of breath\": \"difficulty breathing\",\n",
        "        \"sob \": \"short of breath \",\n",
        "        \"chest pain\": \"pain in the chest\",\n",
        "        \"no evidence of\": \"no clear sign of\",\n",
        "        \"denies\": \"reports no\",\n",
        "    }\n",
        "    out = text\n",
        "    for k, v in repl.items():\n",
        "        out = out.replace(k, v)\n",
        "    return out\n",
        "\n",
        "def reorder_sections(text: str) -> str:\n",
        "    if \"TRIAGE:\" in text and \"RADIOLOGY:\" in text:\n",
        "        triage_part = re.findall(r\"TRIAGE:.*?(?=RADIOLOGY:|$)\", text,\n",
        "                                 flags=re.IGNORECASE)\n",
        "        radiol_part = re.findall(r\"RADIOLOGY:.*?(?=$)\", text,\n",
        "                                 flags=re.IGNORECASE)\n",
        "        if triage_part and radiol_part:\n",
        "            return radiol_part[0] + \" [SEP] \" + triage_part[0]\n",
        "    return text\n",
        "\n",
        "def robustness_check(model, df, labels, tokenizer_or_vocab,\n",
        "                     n_samples=200, model_type=\"bert\"):\n",
        "    model.eval()\n",
        "    idxs = np.random.choice(len(df), size=min(n_samples, len(df)),\n",
        "                            replace=False)\n",
        "    orig_preds, para_preds, reorder_preds = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in idxs:\n",
        "            row = df.iloc[idx]\n",
        "            base_text = row[\"combined_text\"]\n",
        "\n",
        "            texts = {\n",
        "                \"orig\": base_text,\n",
        "                \"para\": simple_paraphrase(base_text),\n",
        "                \"reorder\": reorder_sections(base_text),\n",
        "            }\n",
        "\n",
        "            probs = {}\n",
        "            for key, txt in texts.items():\n",
        "                if model_type == \"bert\":\n",
        "                    enc = tokenizer_or_vocab(\n",
        "                        txt,\n",
        "                        truncation=True,\n",
        "                        max_length=MAX_LEN_BERT,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\",\n",
        "                    )\n",
        "                    ids  = enc[\"input_ids\"].to(device)\n",
        "                    mask = enc[\"attention_mask\"].to(device)\n",
        "                else:\n",
        "                    ids_np, mask_np = text_to_ids(\n",
        "                        txt, tokenizer_or_vocab, MAX_LEN_LSTM\n",
        "                    )\n",
        "                    ids  = torch.tensor(ids_np, dtype=torch.long,\n",
        "                                        device=device).unsqueeze(0)\n",
        "                    mask = torch.tensor(mask_np, dtype=torch.float32,\n",
        "                                        device=device).unsqueeze(0)\n",
        "\n",
        "                logits = model(ids, mask)\n",
        "                prob = torch.sigmoid(logits).item()\n",
        "                probs[key] = prob\n",
        "\n",
        "            orig_preds.append(1 if probs[\"orig\"]   >= 0.5 else 0)\n",
        "            para_preds.append(1 if probs[\"para\"]   >= 0.5 else 0)\n",
        "            reorder_preds.append(1 if probs[\"reorder\"] >= 0.5 else 0)\n",
        "\n",
        "    orig_preds    = np.array(orig_preds)\n",
        "    para_preds    = np.array(para_preds)\n",
        "    reorder_preds = np.array(reorder_preds)\n",
        "\n",
        "    para_consistency    = (orig_preds == para_preds).mean()\n",
        "    reorder_consistency = (orig_preds == reorder_preds).mean()\n",
        "    print(f\"\\n[Robustness] Paraphrasing consistency: {para_consistency:.3f}\")\n",
        "    print(f\"[Robustness] Section reordering consistency: \"\n",
        "          f\"{reorder_consistency:.3f}\")\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.barplot(x=[\"Paraphrase\", \"Reorder\"],\n",
        "                y=[para_consistency, reorder_consistency])\n",
        "    plt.ylim(0,1)\n",
        "    plt.ylabel(\"Consistency rate\")\n",
        "    plt.title(f\"{best_model_name} – Robustness\")\n",
        "    plt.tight_layout()\n",
        "    path_rob = PLOTS_DIR / f\"{best_model_name}_robustness.png\"\n",
        "    plt.savefig(path_rob, dpi=150)\n",
        "    plt.close()\n",
        "    print(f\"[Robustness] Saved bar plot to {path_rob}\")\n",
        "\n",
        "    return para_consistency, reorder_consistency\n",
        "\n",
        "if best_model_name == \"ClinicalBERT\":\n",
        "    rob_para, rob_reorder = robustness_check(\n",
        "        best_model, X_test, y_test, tokenizer,\n",
        "        n_samples=200, model_type=\"bert\"\n",
        "    )\n",
        "else:\n",
        "    rob_para, rob_reorder = robustness_check(\n",
        "        best_model, X_test, y_test, vocab,\n",
        "        n_samples=200, model_type=\"lstm\"\n",
        "    )\n",
        "\n",
        "# Simple rationales for ClinicalBERT only (optional)\n",
        "def extract_rationale_tokens_bert(text, model, tokenizer, top_k=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN_BERT,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        ids  = enc[\"input_ids\"].to(device)\n",
        "        mask = enc[\"attention_mask\"].to(device)\n",
        "        outputs = model.bert(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            output_attentions=True\n",
        "        )\n",
        "        attns    = outputs.attentions[-1]          # [B, heads, T, T]\n",
        "        cls_attn = attns.mean(1)[0, 0]             # [T]\n",
        "        cls_attn = cls_attn * mask[0]\n",
        "        scores   = cls_attn.cpu().numpy()\n",
        "        top_idx  = scores.argsort()[-top_k:][::-1]\n",
        "\n",
        "        tokens = tokenizer.convert_ids_to_tokens(ids[0])\n",
        "        rationale_tokens = [\n",
        "            tokens[i] for i in top_idx\n",
        "            if tokens[i] not in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
        "        ]\n",
        "    return rationale_tokens, top_idx\n",
        "\n",
        "def faithfulness_deletion_bert(text, model, tokenizer, top_k=10):\n",
        "    model.eval()\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN_BERT,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    ids  = enc[\"input_ids\"][0]\n",
        "    mask = enc[\"attention_mask\"][0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(ids.unsqueeze(0).to(device),\n",
        "                       mask.unsqueeze(0).to(device))\n",
        "        base_prob = torch.sigmoid(logits).item()\n",
        "\n",
        "    _, top_idx = extract_rationale_tokens_bert(\n",
        "        text, model, tokenizer, top_k=top_k\n",
        "    )\n",
        "    keep_mask = np.ones_like(ids.cpu().numpy(), dtype=bool)\n",
        "    keep_mask[top_idx] = False\n",
        "\n",
        "    new_ids = ids.cpu().numpy()[keep_mask]\n",
        "    new_ids = new_ids[:MAX_LEN_BERT]\n",
        "    pad_len = MAX_LEN_BERT - len(new_ids)\n",
        "    new_ids = np.concatenate(\n",
        "        [new_ids, [tokenizer.pad_token_id]*pad_len]\n",
        "    )\n",
        "    new_mask = np.array(\n",
        "        [1]*min(len(new_ids)-pad_len, MAX_LEN_BERT) + [0]*pad_len\n",
        "    )\n",
        "\n",
        "    new_ids  = torch.tensor(new_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    new_mask = torch.tensor(new_mask, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        new_logits = model(new_ids, new_mask)\n",
        "        new_prob   = torch.sigmoid(new_logits).item()\n",
        "\n",
        "    drop = base_prob - new_prob\n",
        "    return base_prob, new_prob, drop\n",
        "\n",
        "if best_model_name == \"ClinicalBERT\":\n",
        "    idxs = np.random.choice(len(X_test), size=50, replace=False)\n",
        "    drops = []\n",
        "    for idx in idxs:\n",
        "        txt = X_test.iloc[idx][\"combined_text\"]\n",
        "        base_p, new_p, diff = faithfulness_deletion_bert(\n",
        "            txt, best_model, tokenizer, top_k=10\n",
        "        )\n",
        "        drops.append(diff)\n",
        "    avg_drop = float(np.mean(drops))\n",
        "    print(f\"\\n[Rationales] Avg prob drop after deleting \"\n",
        "          f\"top-10 rationale tokens: {avg_drop:.3f}\")\n",
        "else:\n",
        "    print(\"\\n[Rationales] Rationales implemented only for ClinicalBERT.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuyPVWyxCrR2",
        "outputId": "41598481-cbe8-4fbb-fdf6-ca36b1285be3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Robustness] Paraphrasing consistency: 0.995\n",
            "[Robustness] Section reordering consistency: 0.980\n",
            "[Robustness] Saved bar plot to /content/drive/MyDrive/clinical_project_cpu/deep_models/plots/BiLSTM_robustness.png\n",
            "\n",
            "[Rationales] Rationales implemented only for ClinicalBERT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMvQVkk1DH0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. BiLSTM TEXT EMBEDDINGS"
      ],
      "metadata": {
        "id": "y4oS8UylDxj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHUNK 5 – EXPORT BiLSTM TEXT EMBEDDINGS\n",
        "\n",
        "def export_lstm_embeddings(model, dataset, split_name, batch_size=128):\n",
        "    \"\"\"\n",
        "    Uses model.encode_text() to get fixed-length embeddings\n",
        "    and saves them as NPZ: embeddings, labels, stay_ids.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_emb, all_labels, all_stays = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids  = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            emb = model.encode_text(ids, mask)   # [B, 2H]\n",
        "            all_emb.append(emb.cpu().numpy())\n",
        "            all_labels.append(batch[\"label\"].numpy())\n",
        "            all_stays.append(batch[\"stay_id\"].numpy())\n",
        "\n",
        "    all_emb    = np.vstack(all_emb)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_stays  = np.concatenate(all_stays)\n",
        "\n",
        "    out_path = EMB_DIR / f\"bilstm_embeddings_{split_name}.npz\"\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        embeddings=all_emb,\n",
        "        labels=all_labels,\n",
        "        stay_ids=all_stays,\n",
        "    )\n",
        "    print(f\"[BiLSTM] Saved {split_name} embeddings to {out_path} \"\n",
        "          f\"with shape {all_emb.shape}\")\n",
        "    return out_path\n",
        "\n",
        "path_lstm_train = export_lstm_embeddings(bilstm_model, train_ds_lstm, \"train\")\n",
        "path_lstm_val   = export_lstm_embeddings(bilstm_model, val_ds_lstm,   \"val\")\n",
        "path_lstm_test  = export_lstm_embeddings(bilstm_model, test_ds_lstm,  \"test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWwOO7c9Diva",
        "outputId": "43c1c7ef-2eac-404d-a618-40d95746f1f3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BiLSTM] Saved train embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/bilstm_embeddings_train.npz with shape (141300, 256)\n",
            "[BiLSTM] Saved val embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/bilstm_embeddings_val.npz with shape (30273, 256)\n",
            "[BiLSTM] Saved test embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/bilstm_embeddings_test.npz with shape (29923, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdlKCQGpDizL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ClinicalModernBERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "oDE4iFQNFVMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CHUNK 6 – EXPORT ClinicalModernBERT EMBEDDINGS\n",
        "# ============================================\n",
        "def export_bert_embeddings(model, dataset, split_name, batch_size=64):\n",
        "    \"\"\"\n",
        "    Uses model.encode_text() to get ModernBERT embeddings\n",
        "    and saves them as NPZ: embeddings, labels, stay_ids.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_emb, all_labels, all_stays = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids  = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            emb = model.encode_text(ids, mask)   # [B, H]\n",
        "            all_emb.append(emb.cpu().numpy())\n",
        "            all_labels.append(batch[\"label\"].numpy())\n",
        "            all_stays.append(batch[\"stay_id\"].numpy())\n",
        "\n",
        "    all_emb    = np.vstack(all_emb)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_stays  = np.concatenate(all_stays)\n",
        "\n",
        "    out_path = EMB_DIR / f\"clinicalmodernbert_embeddings_{split_name}.npz\"\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        embeddings=all_emb,\n",
        "        labels=all_labels,\n",
        "        stay_ids=all_stays,\n",
        "    )\n",
        "    print(f\"[ClinicalModernBERT] Saved {split_name} embeddings to {out_path} \"\n",
        "          f\"with shape {all_emb.shape}\")\n",
        "    return out_path\n",
        "\n",
        "path_bert_train = export_bert_embeddings(bert_model, train_ds_bert, \"train\")\n",
        "path_bert_val   = export_bert_embeddings(bert_model, val_ds_bert,   \"val\")\n",
        "path_bert_test  = export_bert_embeddings(bert_model, test_ds_bert,  \"test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07I9B65eFIlc",
        "outputId": "4e8a81aa-6837-4925-f5a9-a070a34bb226"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ClinicalModernBERT] Saved train embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/clinicalmodernbert_embeddings_train.npz with shape (141300, 768)\n",
            "[ClinicalModernBERT] Saved val embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/clinicalmodernbert_embeddings_val.npz with shape (30273, 768)\n",
            "[ClinicalModernBERT] Saved test embeddings to /content/drive/MyDrive/clinical_project_cpu/deep_models/embeddings/clinicalmodernbert_embeddings_test.npz with shape (29923, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1FK-P6IFIpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. ZIP ALL OUTPUTS FOR DOWNLOAD"
      ],
      "metadata": {
        "id": "yRrgOwS4GV33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CHUNK 7 – ZIP ALL OUTPUTS FOR DOWNLOAD\n",
        "# ============================================\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_base = \"/content/clinical_text_deep_models_outputs\"\n",
        "shutil.make_archive(zip_base, \"zip\", BASE_OUT_DIR)\n",
        "\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"Created archive:\", zip_path)\n",
        "\n",
        "# trigger download in Colab\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-005Kk7jGMx_",
        "outputId": "6b2a96a6-87f3-4160-f689-ea6dfc1d9736"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created archive: /content/clinical_text_deep_models_outputs.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e014c325-8a4d-4156-a07e-8075eec32bb5\", \"clinical_text_deep_models_outputs.zip\", 759520010)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lG5rXqyVGM2b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}